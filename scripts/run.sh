#!/bin/bash
# ==============================================
# Transformer Seq2Seq è®­ç»ƒè„šæœ¬
# ç¯å¢ƒè¦æ±‚ï¼š
#   - Python â‰¥ 3.8
#   - PyTorch â‰¥ 2.0
#   - CUDA â‰¥ 11.8
#   - GPU: NVIDIA RTX 3090 (24GB æ˜¾å­˜æ¨è)
# ==============================================

# ----------- åŸºæœ¬è®¾ç½® -----------
EPOCHS=20
LR=5e-4
BATCH_SIZE=64
SAVE_DIR="checkpoints"
WARMUP_STEPS=4000
MAX_STEPS=50000
MAX_SEQ_LEN=100
SEED=42

# ----------- è·¯å¾„è®¾ç½® -----------
PROJECT_ROOT="$(dirname $(dirname "$0"))"
TRAIN_SCRIPT="$PROJECT_ROOT/src/train.py"
DATA_DIR="$PROJECT_ROOT/data/processed"
TOKENIZER="$PROJECT_ROOT/data/tokenizer/iwslt_bpe.model"

# ----------- ç¯å¢ƒæ£€æµ‹ -----------
echo "ğŸ”¥ Checking environment..."
python -c "import torch; print('âœ… PyTorch version:', torch.__version__)"
python -c "import sentencepiece; print('âœ… SentencePiece version:', sentencepiece.__version__)"

# ----------- å¯åŠ¨è®­ç»ƒ -----------
echo "ğŸš€ Starting training..."
python "$TRAIN_SCRIPT" \
    --epochs $EPOCHS \
    --lr $LR \
    --batch-size $BATCH_SIZE \
    --save-dir $SAVE_DIR \
    --warmup-steps $WARMUP_STEPS \
    --max-steps $MAX_STEPS \
    --max-seq-len $MAX_SEQ_LEN \
    --seed $SEED

echo "âœ… Training finished!"
